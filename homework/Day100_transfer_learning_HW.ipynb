{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day100_transfer_learning_HW.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"XLRaONmWwHJy","colab_type":"text"},"cell_type":"markdown","source":["## 作業\n","礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n","\n","\n","最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n","\n","另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"]},{"metadata":{"id":"LGXH275FiPbD","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","def resnet(input_shape, depth=29, num_classes=10):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H3-355qNwHJ4","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import cifar10\n","#from resnet_builder import resnet # 這是從 resnet_builder.py 中直接 import 撰寫好的 resnet 函數\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8lNObzCYg4k5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"c713ee47-565c-46d8-f0ef-919966800cab","executionInfo":{"status":"ok","timestamp":1555709928966,"user_tz":-480,"elapsed":16146,"user":{"displayName":"黃樹瑆","photoUrl":"","userId":"06315090440550119528"}}},"cell_type":"code","source":["# 讀取資料集並作前處理\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","x_train = x_train / 255.\n","x_test = x_test / 255.\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 13s 0us/step\n","x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"metadata":{"id":"xUOmzil_g7_3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3833},"outputId":"a77b329a-8e45-4724-856c-5947067dc8f5","executionInfo":{"status":"ok","timestamp":1555709966578,"user_tz":-480,"elapsed":5324,"user":{"displayName":"黃樹瑆","photoUrl":"","userId":"06315090440550119528"}}},"cell_type":"code","source":["# 建立 ResNet 模型\n","model = resnet(input_shape=(32,32,3)) \n","model.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n","                                                                 conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 32, 16)   1040        activation_7[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 32, 64)   1088        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 32, 32, 64)   0           add_2[0][0]                      \n","                                                                 conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 16, 16, 64)   4160        activation_10[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        add_3[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_15[0][0]                  \n","                                                                 conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 16, 16, 64)   8256        activation_13[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 16, 16, 128)  8320        activation_15[0][0]              \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n","                                                                 conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 16, 16, 64)   8256        activation_16[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 16, 16, 128)  8320        activation_18[0][0]              \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n","                                                                 conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 8, 8, 128)    16512       activation_19[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 8, 8, 256)    33024       add_6[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       activation_21[0][0]              \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_25[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 8, 8, 128)    32896       activation_22[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 8, 8, 128)    147584      activation_23[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 8, 8, 256)    33024       activation_24[0][0]              \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n","                                                                 conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 8, 8, 256)    0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 8, 8, 128)    32896       activation_25[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 8, 8, 256)    33024       activation_27[0][0]              \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 8, 8, 256)    0           add_8[0][0]                      \n","                                                                 conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        add_9[0][0]                      \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_28[0][0]              \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n","==================================================================================================\n","Total params: 849,002\n","Trainable params: 843,786\n","Non-trainable params: 5,216\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"LcLxBKakikb4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1181},"outputId":"2dd40c39-3ceb-4b63-d6c6-03b6793b1d5c","executionInfo":{"status":"ok","timestamp":1555711234846,"user_tz":-480,"elapsed":1258311,"user":{"displayName":"黃樹瑆","photoUrl":"","userId":"06315090440550119528"}}},"cell_type":"code","source":["batch_size = 64 # batch 的大小，如果出現 OOM error，請降低這個值\n","num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n","epochs = 30 # 訓練整個資料集共 30個循環\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(),\n","              metrics=['accuracy'])\n","\n","history = model.fit(x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Train on 50000 samples, validate on 10000 samples\n","Epoch 1/30\n","50000/50000 [==============================] - 51s 1ms/step - loss: 1.8104 - acc: 0.5207 - val_loss: 1.6974 - val_acc: 0.5396\n","Epoch 2/30\n","50000/50000 [==============================] - 41s 825us/step - loss: 1.3218 - acc: 0.6650 - val_loss: 1.4257 - val_acc: 0.6175\n","Epoch 3/30\n","50000/50000 [==============================] - 42s 840us/step - loss: 1.1238 - acc: 0.7240 - val_loss: 1.2329 - val_acc: 0.6790\n","Epoch 4/30\n","50000/50000 [==============================] - 42s 831us/step - loss: 0.9879 - acc: 0.7637 - val_loss: 1.2379 - val_acc: 0.6753\n","Epoch 5/30\n","50000/50000 [==============================] - 42s 833us/step - loss: 0.8991 - acc: 0.7924 - val_loss: 1.1632 - val_acc: 0.7038\n","Epoch 6/30\n","50000/50000 [==============================] - 41s 830us/step - loss: 0.8240 - acc: 0.8168 - val_loss: 1.1048 - val_acc: 0.7236\n","Epoch 7/30\n","50000/50000 [==============================] - 42s 832us/step - loss: 0.7664 - acc: 0.8379 - val_loss: 1.1920 - val_acc: 0.7311\n","Epoch 8/30\n","50000/50000 [==============================] - 41s 824us/step - loss: 0.7195 - acc: 0.8540 - val_loss: 1.2001 - val_acc: 0.7189\n","Epoch 9/30\n","50000/50000 [==============================] - 42s 839us/step - loss: 0.6843 - acc: 0.8657 - val_loss: 1.1294 - val_acc: 0.7314\n","Epoch 10/30\n","50000/50000 [==============================] - 42s 835us/step - loss: 0.6539 - acc: 0.8779 - val_loss: 1.2561 - val_acc: 0.7138\n","Epoch 11/30\n","50000/50000 [==============================] - 42s 832us/step - loss: 0.6227 - acc: 0.8895 - val_loss: 1.1522 - val_acc: 0.7562\n","Epoch 12/30\n","50000/50000 [==============================] - 41s 829us/step - loss: 0.5938 - acc: 0.9011 - val_loss: 1.1819 - val_acc: 0.7406\n","Epoch 13/30\n","50000/50000 [==============================] - 42s 833us/step - loss: 0.5826 - acc: 0.9051 - val_loss: 1.3128 - val_acc: 0.6956\n","Epoch 14/30\n","50000/50000 [==============================] - 41s 825us/step - loss: 0.5598 - acc: 0.9118 - val_loss: 1.2407 - val_acc: 0.7442\n","Epoch 15/30\n","50000/50000 [==============================] - 42s 830us/step - loss: 0.5537 - acc: 0.9162 - val_loss: 1.2454 - val_acc: 0.7425\n","Epoch 16/30\n","50000/50000 [==============================] - 41s 828us/step - loss: 0.5433 - acc: 0.9199 - val_loss: 1.2540 - val_acc: 0.7514\n","Epoch 17/30\n","50000/50000 [==============================] - 41s 826us/step - loss: 0.5293 - acc: 0.9257 - val_loss: 1.9842 - val_acc: 0.6705\n","Epoch 18/30\n","50000/50000 [==============================] - 42s 830us/step - loss: 0.5244 - acc: 0.9283 - val_loss: 1.5070 - val_acc: 0.7090\n","Epoch 19/30\n","50000/50000 [==============================] - 41s 830us/step - loss: 0.5189 - acc: 0.9297 - val_loss: 1.2771 - val_acc: 0.7518\n","Epoch 20/30\n","50000/50000 [==============================] - 41s 819us/step - loss: 0.5122 - acc: 0.9322 - val_loss: 1.2364 - val_acc: 0.7625\n","Epoch 21/30\n","50000/50000 [==============================] - 41s 826us/step - loss: 0.5065 - acc: 0.9357 - val_loss: 1.7710 - val_acc: 0.6675\n","Epoch 22/30\n","50000/50000 [==============================] - 41s 819us/step - loss: 0.5024 - acc: 0.9367 - val_loss: 1.2052 - val_acc: 0.7532\n","Epoch 23/30\n","50000/50000 [==============================] - 41s 827us/step - loss: 0.4956 - acc: 0.9394 - val_loss: 1.3626 - val_acc: 0.7504\n","Epoch 24/30\n","50000/50000 [==============================] - 41s 819us/step - loss: 0.4918 - acc: 0.9409 - val_loss: 1.3193 - val_acc: 0.7640\n","Epoch 25/30\n","50000/50000 [==============================] - 42s 834us/step - loss: 0.4861 - acc: 0.9435 - val_loss: 1.2902 - val_acc: 0.7524\n","Epoch 26/30\n","50000/50000 [==============================] - 41s 819us/step - loss: 0.4878 - acc: 0.9424 - val_loss: 1.6022 - val_acc: 0.7114\n","Epoch 27/30\n","50000/50000 [==============================] - 41s 824us/step - loss: 0.4850 - acc: 0.9423 - val_loss: 1.5413 - val_acc: 0.6984\n","Epoch 28/30\n","50000/50000 [==============================] - 41s 815us/step - loss: 0.4801 - acc: 0.9450 - val_loss: 1.5024 - val_acc: 0.7500\n","Epoch 29/30\n","50000/50000 [==============================] - 41s 819us/step - loss: 0.4849 - acc: 0.9432 - val_loss: 1.2631 - val_acc: 0.7692\n","Epoch 30/30\n","50000/50000 [==============================] - 41s 817us/step - loss: 0.4787 - acc: 0.9462 - val_loss: 1.3250 - val_acc: 0.7478\n","Test loss: 1.3249782298088073\n","Test accuracy: 0.7478\n"],"name":"stdout"}]},{"metadata":{"id":"SJsEpiAzmTaC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1092},"outputId":"26c7ffe7-faeb-48d5-c677-27b185e1ff6e","executionInfo":{"status":"ok","timestamp":1555713995003,"user_tz":-480,"elapsed":2712859,"user":{"displayName":"黃樹瑆","photoUrl":"","userId":"06315090440550119528"}}},"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","data_generator = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True)\n","\n","history = model.fit_generator(data_generator.flow(x_train, y_train, batch_size=batch_size),\n","                              epochs=epochs,\n","                              steps_per_epoch=int(50000/32),\n","                              verbose=1,\n","                              validation_data=(x_test, y_test))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 1.0294 - acc: 0.7571 - val_loss: 1.6249 - val_acc: 0.6393\n","Epoch 2/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.9113 - acc: 0.7859 - val_loss: 1.4444 - val_acc: 0.6782\n","Epoch 3/30\n","1562/1562 [==============================] - 90s 57ms/step - loss: 0.8578 - acc: 0.8017 - val_loss: 1.0276 - val_acc: 0.7671\n","Epoch 4/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.8180 - acc: 0.8108 - val_loss: 0.8522 - val_acc: 0.7979\n","Epoch 5/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.7847 - acc: 0.8181 - val_loss: 1.1366 - val_acc: 0.7437\n","Epoch 6/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.7599 - acc: 0.8255 - val_loss: 0.9767 - val_acc: 0.7744\n","Epoch 7/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.7390 - acc: 0.8302 - val_loss: 0.9349 - val_acc: 0.7920\n","Epoch 8/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.7200 - acc: 0.8362 - val_loss: 0.9761 - val_acc: 0.7781\n","Epoch 9/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.7029 - acc: 0.8413 - val_loss: 0.8158 - val_acc: 0.8196\n","Epoch 10/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.6919 - acc: 0.8431 - val_loss: 1.0477 - val_acc: 0.7617\n","Epoch 11/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.6782 - acc: 0.8480 - val_loss: 0.9246 - val_acc: 0.7933\n","Epoch 12/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.6681 - acc: 0.8503 - val_loss: 1.0435 - val_acc: 0.7710\n","Epoch 13/30\n","1562/1562 [==============================] - 89s 57ms/step - loss: 0.6571 - acc: 0.8533 - val_loss: 0.8046 - val_acc: 0.8134\n","Epoch 14/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.6498 - acc: 0.8556 - val_loss: 1.0714 - val_acc: 0.7646\n","Epoch 15/30\n","1562/1562 [==============================] - 93s 60ms/step - loss: 0.6406 - acc: 0.8587 - val_loss: 0.7269 - val_acc: 0.8382\n","Epoch 16/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.6324 - acc: 0.8618 - val_loss: 0.8389 - val_acc: 0.8095\n","Epoch 17/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.6310 - acc: 0.8614 - val_loss: 0.8286 - val_acc: 0.8133\n","Epoch 18/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.6122 - acc: 0.8679 - val_loss: 0.7730 - val_acc: 0.8265\n","Epoch 19/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.6115 - acc: 0.8679 - val_loss: 0.7499 - val_acc: 0.8340\n","Epoch 20/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.6059 - acc: 0.8696 - val_loss: 1.0568 - val_acc: 0.7659\n","Epoch 21/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.6013 - acc: 0.8706 - val_loss: 0.7773 - val_acc: 0.8247\n","Epoch 22/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.5926 - acc: 0.8730 - val_loss: 0.7294 - val_acc: 0.8382\n","Epoch 23/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.5923 - acc: 0.8726 - val_loss: 0.8068 - val_acc: 0.8220\n","Epoch 24/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.5840 - acc: 0.8750 - val_loss: 0.7328 - val_acc: 0.8441\n","Epoch 25/30\n","1562/1562 [==============================] - 91s 58ms/step - loss: 0.5822 - acc: 0.8750 - val_loss: 0.8196 - val_acc: 0.8200\n","Epoch 26/30\n","1562/1562 [==============================] - 90s 57ms/step - loss: 0.5781 - acc: 0.8768 - val_loss: 0.7567 - val_acc: 0.8322\n","Epoch 27/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.5713 - acc: 0.8800 - val_loss: 1.0373 - val_acc: 0.7889\n","Epoch 28/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.5715 - acc: 0.8792 - val_loss: 0.8168 - val_acc: 0.8129\n","Epoch 29/30\n","1562/1562 [==============================] - 90s 58ms/step - loss: 0.5670 - acc: 0.8799 - val_loss: 0.7488 - val_acc: 0.8400\n","Epoch 30/30\n","1562/1562 [==============================] - 89s 57ms/step - loss: 0.5630 - acc: 0.8810 - val_loss: 0.9303 - val_acc: 0.7948\n","Test loss: 0.9303182140350342\n","Test accuracy: 0.7948\n"],"name":"stdout"}]},{"metadata":{"id":"UjcE9Cufne8y","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}